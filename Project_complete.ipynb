{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkdoyun/google_colab/blob/master/Project_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC3JvmVny0_c",
        "outputId": "357cc5a3-98c5-400c-e717-b0187ba32762"
      },
      "source": [
        "# ctrl+f9 -> 모두 실행\n",
        "# ctrl+f10 -> 이후 셀 실행\n",
        "\n",
        "# google 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytHP3bfv2HkV",
        "outputId": "ffeb9346-51c9-4571-8737-812b9aa1e5b8"
      },
      "source": [
        "pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 100.1 MB 9.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566185 sha256=b038e7875cb7484db6682370c6380f53f94a3b94ef1f4ef70a51ad69e434bc9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kow0e_m9Lu3"
      },
      "source": [
        "# 웹캠 사진 출력 함수 take_photo()\n",
        "# 캡처 버튼 누르면 -> 'photo.jpg'로 저장\n",
        "# ctrl+f10 -> 이후 셀 실행\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q76Njp0F9Zm7"
      },
      "source": [
        "# 웹캠으로 사진 찍어서 얼굴 부분만 크롭\n",
        "# 해당 이미지 저장하기 (photo_trim.jpg)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import face_recognition\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# 사진 찍기\n",
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))\n",
        "\n",
        "# 사진에 네모박스 치기\n",
        "import cv2\n",
        "import numpy as np\n",
        "import face_recognition\n",
        "\n",
        "img1 = face_recognition.load_image_file('photo.jpg')\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# face_recognition이 검출한 얼굴의 위치 가져옴\n",
        "# 한 이미지에 사람 많다면 각 구역에 대한 위치 반환\n",
        "\n",
        "faceLoc = face_recognition.face_locations(img1)[0]\n",
        "encodeImg1 = face_recognition.face_encodings(img1)[0] # 얼굴 구역 내 세밀한 특징 인코딩\n",
        "# 특징 많이 보유 -> 그 사람이라 판단, 다른 사진들도 학습 & 예측 가능\n",
        "# cv2.rectangle(img1, (faceLoc[3], faceLoc[0]), (faceLoc[1], faceLoc[2]), (255, 0, 255), 2) # 사각형 얼굴에 표시\n",
        "# cv2.rectangle(사진, start_point(시작점 x, y), end_point, color, thickness)\n",
        "\n",
        "# cv2_imshow(img1) # 사진 출력\n",
        "\n",
        "# 이미지 얼굴 부분 자르고 따로 저장\n",
        "x = faceLoc[3]; y = faceLoc[0] # 자르고 싶은 지점의 x좌표, y좌표\n",
        "w = abs(faceLoc[1]-faceLoc[3]); h = abs(faceLoc[2]-faceLoc[0]) # x로부터 width, y로부터 height 지정\n",
        "\n",
        "# 크롭 범위 확대\n",
        "# x = int(x / 1.8); y = int(y / 1.8)\n",
        "# w = int(w * 1.8); h = int(h * 1.8)\n",
        "\n",
        "img_trim = img1[y:y + h, x:x+w] # trim한 결과를 img_trim에 넣는다\n",
        "cv2.imwrite('photo_trim.jpg', img_trim) # 저장\n",
        "\n",
        "img_trim = cv2.imread('photo_trim.jpg', cv2.IMREAD_GRAYSCALE) # 흑백 처리해서 다시 불러옴\n",
        "cv2_imshow(img_trim) # display\n",
        "\n",
        "cv2.imwrite('photo_trim.jpg', img_trim) # 흑백으로 다시 저장\n",
        "\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vduKpVsXZOe3"
      },
      "source": [
        "# Neural Style Transfer 시작\n",
        "\n",
        "import tensorflow as tf\n",
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor * 255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor) > 3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return PIL.Image.fromarray(tensor)\n",
        "\n",
        "# 스타일 참조 이미지와 콘텐츠 이미지 선택\n",
        "\n",
        "# 이미지 다운로드 함수\n",
        "content_path = 'photo_trim.jpg' # 자르고 생성한 사진\n",
        "style_path = '/content/drive/MyDrive/Colab_Notebooks/face_style3.png' # 스타일 사진\n",
        "# style_path = '/content/drive/MyDrive/Colab_Notebooks/face_style2.jpg'\n",
        "\n",
        "# 입력 시각화\n",
        "# 이미지 불러오는 함수 정의, 최대 이미지 크기를 512개의 픽셀로 제한\n",
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img\n",
        "\n",
        "# 이미지 출력 위한 함수 정의\n",
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "  \n",
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af8OTQBjIkKk"
      },
      "source": [
        "# tf-hub 통한 빠른 스타일 전이 시험 코드\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')\n",
        "stylized_image = hub_module(tf.constant(content_image),  tf.constant(style_image))[0]\n",
        "tensor_to_image(stylized_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfYEWqJIJVHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5055cc92-c938-477b-db64-fe8283b89868"
      },
      "source": [
        "# 콘텐츠와 스타일 표현 정의\n",
        "# 모델의 몇가지 중간층 살펴봄\n",
        "# 입력층부터 ~> 처음 몇개 층 : 저차원적 특성에 반응 (선분, 질감..)\n",
        "# 네트워크가 깊어질수록 ~> 최종 몇개 층 : 고차원적 특성에 반응 (바퀴, 눈..)\n",
        "# VGG19 네트워크 구조 사용 : 사전학습된 이미지 분류 네트워크\n",
        "# 입력 이미지 주어지면 -> 스타일 전이 알고리즘은 이 중간층에서 content_img와 style_img 해당하는 타깃 표현 일치시키려고 시도함\n",
        "\n",
        "# VGG19 모델 불러오고, 작동 여부 확인 위해 이미지에 적용\n",
        "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
        "x = tf.image.resize(x, (224, 224))\n",
        "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "prediction_probabilities = vgg(x)\n",
        "prediction_probabilities.shape\n",
        "\n",
        "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
        "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]\n",
        "\n",
        "# 분류층 제외한 VGG19 모델 불러오고, 각 층의 이름 출력\n",
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\"\"\"\n",
        "print()\n",
        "for layer in vgg.layers:\n",
        "  print(layer.name)\n",
        "\"\"\"\n",
        "# 이미지의 스타일과 콘텐츠 나타내기 위한 모델의 중간층 선택\n",
        "content_layers = ['block5_conv2']\n",
        "\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
            "574717952/574710816 [==============================] - 7s 0us/step\n",
            "574726144/574710816 [==============================] - 7s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 0s 0us/step\n",
            "49152/35363 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuZd1RN1LalK"
      },
      "source": [
        "# 스타일과 콘텐츠를 위한 중간층\n",
        "# 중간 출력으로 어떻게 스타일과 콘텐츠 표현 정의?\n",
        "# 고수준 이미지 분류 -> 네트워크가 반드시 이미지 이해해야 함\n",
        "# 미가공 이미지 -> [이미지 내 특성들에 대한 복합적 이해로 변환]할 수 있는 내부 표현 만드는 작업 포함\n",
        "# 합성곱 신경망의 일반화 쉽게 가능 -> 노이즈에 상관없이 불변성과 특징 포착 가능\n",
        "# 모델은 출력 중간 어딘가에서 복합 특성 추출기 역할 수행\n",
        "# 모델의 중간층 접근 -> 입력 이미지의 콘텐츠와 스타일 추출\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "lRgKyjtuWKua",
        "outputId": "edd86fde-6bd7-4ee5-fe8a-9f91c4ba85e5"
      },
      "source": [
        "# 모델 만들기\n",
        "# tf.keras.applications 제공 모델 -> 중간층에 쉽게 접근 가능\n",
        "# 모델 정의하려면 -> 모델의 입출력 지정\n",
        "# model = Model(inputs, outputs)\n",
        "\n",
        "# 결과 배열로 출력하는 VGG19 model return\n",
        "def vgg_layers(layer_names):\n",
        "  # 이미지넷 데이터셋에 사전학습된 VGG 모델 불러옴\n",
        "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg.trainable = False\n",
        "\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = tf.keras.Model([vgg.input], outputs)\n",
        "  return model\n",
        "\n",
        "style_extractor = vgg_layers(style_layers)\n",
        "style_outputs = style_extractor(style_image*255)\n",
        "\n",
        "# 각 층의 출력에 대한 통계량\n",
        "\"\"\"\n",
        "for name, output in zip(style_layers, style_outputs):\n",
        "  print(name)\n",
        "  print(\"크기 \" , output.numpy().shape)\n",
        "  print(\"최솟값 \" , output.numpy().min())\n",
        "  print(\"최댓값 \" , output.numpy().max())\n",
        "  print(\"평균 \" , output.numpy().mean)\n",
        "  print()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor name, output in zip(style_layers, style_outputs):\\n  print(name)\\n  print(\"크기 \" , output.numpy().shape)\\n  print(\"최솟값 \" , output.numpy().min())\\n  print(\"최댓값 \" , output.numpy().max())\\n  print(\"평균 \" , output.numpy().mean)\\n  print()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "sp_Y-y5BXLHI",
        "outputId": "2a0ef139-8375-46d7-9e34-97c9b939f1d9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 스타일 계산\n",
        "def gram_matrix(input_tensor):\n",
        "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "  return result/(num_locations)\n",
        "\n",
        "# 스타일과 콘텐츠 추출\n",
        "# 스타일과 콘텐츠 텐서 반환하는 모델 생성\n",
        "# StyleContentModel 클래스 생성\n",
        "class StyleContentModel(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleContentModel, self).__init__()\n",
        "    self.vgg = vgg_layers(style_layers + content_layers)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.vgg.trainable = False\n",
        "\n",
        "  def call(self, inputs): # 0~1 사이 실수 입력\n",
        "    inputs = inputs*255.0\n",
        "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
        "                                      outputs[self.num_style_layers:])\n",
        "    \n",
        "    style_outputs = [gram_matrix(style_output)\n",
        "                    for style_output in style_outputs]\n",
        "\n",
        "    content_dict = {content_name:value for content_name, value in zip(self.content_layers, content_outputs)}\n",
        "    style_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "    return {'content':content_dict, 'style':style_dict}\n",
        "\n",
        "# 이미지 입력으로 주어졌을 때, style_layers의 스타일과 content_layers의 콘텐츠에 대한 그람 행렬 출력\n",
        "# 추출기\n",
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "results = extractor(tf.constant(content_image))\n",
        "\"\"\"\n",
        "print('style')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "  print(\" \", name)\n",
        "  print(\"  크기 \", output.numpy().shape)\n",
        "  print(\"  최솟값 \", output.numpy().shape)\n",
        "  print(\"  최댓값 \", output.numpy().shape)\n",
        "  print(\"  평균 \", output.numpy().shape)\n",
        "  print()\n",
        "\n",
        "print('content')\n",
        "for name, output in sorted(results['content'].items()):\n",
        "  print(\" \", name)\n",
        "  print(\"  크기 \", output.numpy().shape)\n",
        "  print(\"  최솟값 \", output.numpy().shape)\n",
        "  print(\"  최댓값 \", output.numpy().shape)\n",
        "  print(\"  평균 \", output.numpy().shape)\n",
        "  print()\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(\\'style\\')\\nfor name, output in sorted(results[\\'style\\'].items()):\\n  print(\" \", name)\\n  print(\"  크기 \", output.numpy().shape)\\n  print(\"  최솟값 \", output.numpy().shape)\\n  print(\"  최댓값 \", output.numpy().shape)\\n  print(\"  평균 \", output.numpy().shape)\\n  print()\\n\\nprint(\\'content\\')\\nfor name, output in sorted(results[\\'content\\'].items()):\\n  print(\" \", name)\\n  print(\"  크기 \", output.numpy().shape)\\n  print(\"  최솟값 \", output.numpy().shape)\\n  print(\"  최댓값 \", output.numpy().shape)\\n  print(\"  평균 \", output.numpy().shape)\\n  print()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5WVGwyma_46"
      },
      "source": [
        "# 경사하강법 실행\n",
        "# 스타일과 콘텐츠 추출기로 스타일 전이 알고리즘 구현\n",
        "# 타깃에 대한 입력 이미지의 평균 제곱 오차 계산, 오차값들의 가중합 구함\n",
        "\n",
        "# 스타일과 콘텐츠의 타깃값 지정\n",
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']\n",
        "\n",
        "# 최적화시킬 이미지를 담을 tf.Variable 정의, 콘텐츠 이미지로 초기화\n",
        "# ***이때 tf.Variable은 콘텐츠 이미지와 크기 동일해야 함!! ***\n",
        "\n",
        "image = tf.Variable(content_image)\n",
        "\n",
        "# 픽셀 값 -> 실수이므로 0~1 사이로 클리핑(범위 제한)\n",
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min = 0.0, clip_value_max = 1.0)\n",
        "\n",
        "# 옵티마이저 생성\n",
        "opt = tf.optimizers.Adam(learning_rate = 0.02, beta_1 = 0.99, epsilon = 1e-1)\n",
        "\n",
        "# 전체 오차 = 콘텐츠와 스타일 오차의 가중합\n",
        "style_weight = 1e-2\n",
        "content_weight = 1e4\n",
        "\n",
        "def style_content_loss(outputs): # 콘텐츠와 스타일 오차의 가중합\n",
        "  style_outputs = outputs['style']\n",
        "  content_outputs = outputs['content']\n",
        "  style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n",
        "  for name in style_outputs.keys()])\n",
        "\n",
        "  style_loss *= style_weight / num_style_layers\n",
        "\n",
        "  content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n",
        "  for name in content_outputs.keys()])\n",
        "\n",
        "  content_loss *= content_weight / num_content_layers\n",
        "  loss = style_loss + content_loss\n",
        "  return style_loss\n",
        "\n",
        "# tf.GradientTape 사용해 이미지 업데이트\n",
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))\n",
        "\n",
        "# 최적화 진행\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"훈련 스텝 : {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"전체 소요 시간 : {:.1f}\".format(end-start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llV9tbWykgXp"
      },
      "source": [
        "# 총 변위 손실 (total variation loss)\n",
        "# 이 구현 방식의 단점 : 많은 high frequency artifact(고주파 아티팩, 고주파 인공물)이 생겨남\n",
        "# 아티팩 생성 줄이기 위해 -> 이미지의 고주파 구성 요소에 대한 regularization(정규화) 수행\n",
        "# 이 변형된 오차값 : 총 변위 손실\n",
        "\n",
        "def high_pass_x_y(image):\n",
        "  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
        "  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n",
        "\n",
        "  return x_var, y_var\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas : Original\")\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas : Original\")\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(image)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas : Styled\")\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas : Styled\")\n",
        "\n",
        "# 위 이미지들 -> 고주파 구성 요소가 늘어났다는 것 보여줌\n",
        "# 고주파 구성 요소 =  경계선 탐지기 일종\n",
        "# Sobel edge detector(소벨 경계선 탐지기) 이용하면 위와 유사한 출력 얻을 수 있음\n",
        "\n",
        "\"\"\"\n",
        "plt.figure(figsize=(14, 10))\n",
        "sobel = tf.image.sobel_edges(content_image)\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")\n",
        "\"\"\"\n",
        "\n",
        "# 정규화 오차 -> 각 값의 절대값의 합\n",
        "# 이거 구현할 필요 X -> 텐서플로에 내장\n",
        "\"\"\"\n",
        "def total_variation_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n",
        "\n",
        "total_variation_loss(image).numpy()\n",
        "\"\"\"\n",
        "\n",
        "tf.image.total_variation(image).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoqDml0gnUNx"
      },
      "source": [
        "# 최적화\n",
        "# total_variation_loss 위한 가중치 정의\n",
        "total_variation_weight = 30\n",
        "\n",
        "# 가중치를 train_step()에서 사용\n",
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "    loss += total_variation_weight * tf.image.total_variation(image)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))\n",
        "\n",
        "# 최적화 변수를 다시 초기화\n",
        "image = tf.Variable(content_image)\n",
        "\n",
        "# 최적화 수행\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"훈련 스텝 : {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"전체 소요 시간 : {:.1f}\".format(end-start))\n",
        "\n",
        "# 결과물 파일 저장\n",
        "file_name = 'photo_complete.jpg'\n",
        "tensor_to_image(image).save(file_name)\n",
        "\n",
        "# PC 저장\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download(file_name)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}